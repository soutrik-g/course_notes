# Expected Value
## Definition
The ***expected value*** (or *expectation* or *mean*) of the random quantitative variable $X(s)$ on the sample space $S$ is equal to
$$E(X) = \sum_{s \in S} p(s) X(s)$$

## Theorem 1:
If $X$ is a random variable and $p(X = r)$ is the probability that $X = r$, so that

$$\begin{align*}
	p(X = r) &= \sum_{s \in S, X(s) = r} p(s), \text{then} \\
	E(X) &= \sum_{r \in X(S)} p(X = r) r
\end{align*}$$

### Proof
Suppose that $X$ is a random variable with range $X(S)$ and let $p(X = r)$ is the sum of the probabilities of the outcomes $s$ such that $X(s) = r$. Hence,
$$E(X) = \sum_{r \in X(S)} p(X = r) r$$

## Theorem 2:
The expected number of successes when $n$ mutually independent Bernoulli trials are performed is $np$, where $p$ is the probability of successes on each trial.

### Proof
![[Pasted image 20220321104135.png]]

![[Pasted image 20220321104153.png]]
*Second line should be $p+q$.*

# Linearity of Expectations
## Theorem 3
If $X_i, i = 1, 2, \dots, n$ with $n \in \mathbb Z^+$, are random variables on $S$, and if $a$ and $b$ are real numbers, then
![[Pasted image 20220321105019.png]]

# Variance
The ***deviation*** of $X$ at $s \in S$ is $X(s) - E(X)$, the difference between the value of $X$ and the mean of $X$.

## Definition
Let $X$ be a random variable on the sample space $S$. The ***variance*** of $X$, denoted by $V(X)$ is
$$V(X) = \sum_{s \in S}(X(s) - E(X))^2 p(s)$$
That is $V(X)$ is the weighted averaged of the deviation of $X$. The standard deviation of $X$, denoted $\sigma(X)$ is defined to be $\sqrt{V(X)}$.

## Theorem
If $X$ is a random variable on a sample space $S$, then
$$V(X) = E(X^2) - E(X)^2$$

## Corollary
If $X$ is a random variable on a sample space $S$ and $E(X) = \mu$, then 
$$V(X) = E((X - \mu)^2)$$

## Bienaym√©'s Formula
![[Pasted image 20220321111705.png]]

## Chebyshev's Inequality
![[Pasted image 20220321111811.png]]